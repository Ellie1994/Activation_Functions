# Activation_Functions
The code is additional to the article on [my blog](https://siegel.work/blog/) and presents some popular **activation functions**. Sigmoid and tanH became less popular in last years. On the other hand, ReLu (*Rectified Linear Unit*) gained a lot popularity recently.
